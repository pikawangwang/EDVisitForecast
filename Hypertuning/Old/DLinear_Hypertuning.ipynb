{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PIZD1vnM08zL"
   },
   "outputs": [],
   "source": [
    "# Hyperparameter tuning and optimization\n",
    "import optuna\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback, TuneReportCheckpointCallback\n",
    "\n",
    "# PyTorch Lightning and callbacks\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, Callback, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Metrics\n",
    "from torchmetrics import MeanAbsoluteError, MeanAbsolutePercentageError, MetricCollection\n",
    "\n",
    "# Darts (Time series forecasting)\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.models import DLinearModel, LightGBMModel, BlockRNNModel, TiDEModel\n",
    "\n",
    "# Data handling and preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorboard\n",
    "\n",
    "# System utilities\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRL7Bd-51F6b"
   },
   "source": [
    "Load Data / Spilt Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7JB4QAplQd-T",
    "outputId": "844a32da-ecac-4a6c-815f-ca6d65d0ac3d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian11\\AppData\\Local\\Temp\\ipykernel_5416\\4252752960.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df.loc[:, 'No_scaled'] = scaler.transform(train_df[['No']])\n",
      "C:\\Users\\ian11\\AppData\\Local\\Temp\\ipykernel_5416\\4252752960.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_df.loc[:, 'No_scaled'] = scaler.transform(val_df[['No']])\n",
      "C:\\Users\\ian11\\AppData\\Local\\Temp\\ipykernel_5416\\4252752960.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df.loc[:, 'No_scaled'] = scaler.transform(test_df[['No']])  # 用相同的scaler转换测试集以避免数据泄露\n"
     ]
    }
   ],
   "source": [
    "# 步骤1: 加载CSV文件\n",
    "df = pd.read_csv('../DataSet/EDvisitfileC.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# 确保'date'列是DateTime类型\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# 分割数据集为训练集、验证集和测试集（假设您已经根据时间排序）\n",
    "train_end = 3237            #L: 3362, T:3372, Ka:3208, Ke:3274, Y:2557, C: 3237\n",
    "val_end = 3602              #L: 3727, T:3737, Ka:3573, Ke:3639, Y:2822, C: 3602\n",
    "\n",
    "# Split the DataFrame\n",
    "train_df = df.iloc[:train_end]\n",
    "val_df = df.iloc[train_end:val_end]\n",
    "test_df = df.iloc[val_end:]\n",
    "\n",
    "# 步骤2: 使用MinMaxScaler缩放数据\n",
    "# 定义并拟合scaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_df[['No']])  # 只用训练数据拟合scaler\n",
    "\n",
    "# 缩放训练集和验证集\n",
    "train_df.loc[:, 'No_scaled'] = scaler.transform(train_df[['No']])\n",
    "val_df.loc[:, 'No_scaled'] = scaler.transform(val_df[['No']])\n",
    "test_df.loc[:, 'No_scaled'] = scaler.transform(test_df[['No']])  # 用相同的scaler转换测试集以避免数据泄露\n",
    "\n",
    "# 转换为TimeSeries对象\n",
    "train_series = TimeSeries.from_dataframe(train_df, value_cols='No_scaled')\n",
    "val_series = TimeSeries.from_dataframe(val_df, value_cols='No_scaled')\n",
    "test_series = TimeSeries.from_dataframe(test_df, value_cols='No_scaled')\n",
    "\n",
    "# 原始数据转换为TimeSeries对象，如果需要\n",
    "train_series_origin = TimeSeries.from_dataframe(train_df, value_cols='No')\n",
    "val_series_origin = TimeSeries.from_dataframe(val_df, value_cols='No')\n",
    "test_series_origin = TimeSeries.from_dataframe(test_df, value_cols='No')\n",
    "\n",
    "# 选择需要的列创建多变量时间序列(都是one hot coding)\n",
    "columns = ['Dayoff', 'Mon', 'Tue', 'Wed', 'Thr', 'Fri', 'Sat', 'Sun', 'Dayscaled', 'NewYear', '3Lock', \n",
    "           'Outbreak','COVID19', 'Jan',\t'Feb', 'Mar', 'Apr', 'Mar', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov','Dec'\n",
    "          ]\n",
    "df_multivariate = df[columns]\n",
    "\n",
    "# 将DataFrame转换为多变量时间序列\n",
    "ED_covariates = TimeSeries.from_dataframe(df_multivariate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QDhOOMG1KF7"
   },
   "source": [
    "Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossLoggingCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.val_losses = []  # To store validation losses\n",
    "        self.train_losses = []  # To store training losses\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        val_loss = trainer.callback_metrics[\"val_loss\"].item()\n",
    "        self.val_losses.append(val_loss)\n",
    "        print(f\"Epoch {trainer.current_epoch}: val_loss={val_loss}\")\n",
    "        # Updated report call\n",
    "        train.report({\"loss\": val_loss})  # Report the validation loss to Ray Train\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module, unused=None):\n",
    "        if \"train_loss\" in trainer.callback_metrics:\n",
    "            train_loss = trainer.callback_metrics[\"train_loss\"].item()\n",
    "            self.train_losses.append(train_loss)\n",
    "            print(f\"Epoch {trainer.current_epoch}: train_loss={train_loss}\")\n",
    "loss_logging_callback = LossLoggingCallback()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "W9kikpPwzVdk"
   },
   "outputs": [],
   "source": [
    "# Create the model using model_args from Ray Tune\n",
    "def train_model(model_args, callbacks, train, val):\n",
    "    torch_metrics = MetricCollection([MeanAbsolutePercentageError(), MeanAbsoluteError()])\n",
    "    \n",
    "    # Customize the ModelCheckpoint callback\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"checkpoints\",\n",
    "        filename=\"{epoch}-{val_loss:.2f}\",\n",
    "        every_n_epochs=5,\n",
    "    )\n",
    "    \n",
    "    model = DLinearModel(\n",
    "        input_chunk_length=60,\n",
    "        output_chunk_length=7,\n",
    "        pl_trainer_kwargs={\"callbacks\": callbacks, \"enable_progress_bar\": False},\n",
    "        log_tensorboard=True,\n",
    "        **model_args)\n",
    "\n",
    "    model.fit(\n",
    "    series=[train_series],\n",
    "    past_covariates=[ED_covariates],\n",
    "    val_series=[val_series],\n",
    "    val_past_covariates=[ED_covariates]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "e2pX_rH51IHe"
   },
   "outputs": [],
   "source": [
    "# set up ray tune callback\n",
    "config = {\n",
    "    'kernel_size': tune.randint(5, 100),\n",
    "    'lr_scheduler_kwargs': tune.uniform(0, 0.01),\n",
    "}\n",
    "\n",
    "# earlystopping\n",
    "my_stopper = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    min_delta=0.001,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "\n",
    "tune_callback = TuneReportCheckpointCallback(\n",
    "    {\n",
    "        \"loss\": \"val_loss\",\n",
    "    },\n",
    "    on=\"validation_end\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# define the hyperparameter space\n",
    "\n",
    "reporter = CLIReporter(\n",
    "    parameter_columns=list(config.keys()),\n",
    "    metric_columns=[\"loss\", \"MAPE\", \"training_iteration\"],\n",
    ")\n",
    "\n",
    "optuna_search = OptunaSearch(metric=\"loss\", mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4L522X41XIL",
    "outputId": "4df26e54-fe79-499c-938d-b376c0e22c76"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 21:42:51,843\tINFO worker.py:1752 -- Started a local Ray instance.\n",
      "2024-09-23 21:42:54,553\tINFO tune.py:263 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n",
      "2024-09-23 21:42:54,556\tINFO tune.py:613 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "[I 2024-09-23 21:42:54,580] A new study created in memory with name: optuna\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-09-23 21:42:54 (running for 00:00:00.28)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 640.000: None | Iter 320.000: None | Iter 160.000: None | Iter 80.000: None | Iter 40.000: None | Iter 20.000: None | Iter 10.000: None | Iter 5.000: None\n",
      "Logical resource usage: 1.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/ian11/AppData/Local/Temp/ray/session_2024-09-23_21-42-49_373180_5416/artifacts/2024-09-23_21-42-54/tune_darts/driver_artifacts\n",
      "Number of trials: 1/30 (1 PENDING)\n",
      "+----------------------+----------+-------+---------------+-----------------------+\n",
      "| Trial name           | status   | loc   |   kernel_size |   lr_scheduler_kwargs |\n",
      "|----------------------+----------+-------+---------------+-----------------------|\n",
      "| train_model_a8f6ff1a | PENDING  |       |            33 |            0.00126183 |\n",
      "+----------------------+----------+-------+---------------+-----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[33m(raylet)\u001b[0m   File \"C:\\Users\\ian11\\anaconda3\\envs\\EDforecast\\lib\\site-packages\\ray\\_private\\workers\\setup_worker.py\", line 33, in <module>\n",
      "\u001b[33m(raylet)\u001b[0m     runtime_env_context.exec_worker(remaining_args, Language.Value(args.language))\n",
      "\u001b[33m(raylet)\u001b[0m   File \"C:\\Users\\ian11\\anaconda3\\envs\\EDforecast\\lib\\site-packages\\ray\\_private\\runtime_env\\context.py\", line 86, in exec_worker\n",
      "\u001b[33m(raylet)\u001b[0m     subprocess.Popen(cmd, shell=True).wait()\n",
      "\u001b[33m(raylet)\u001b[0m   File \"C:\\Users\\ian11\\anaconda3\\envs\\EDforecast\\lib\\subprocess.py\", line 858, in __init__\n",
      "\u001b[33m(raylet)\u001b[0m     self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "\u001b[33m(raylet)\u001b[0m   File \"C:\\Users\\ian11\\anaconda3\\envs\\EDforecast\\lib\\subprocess.py\", line 1327, in _execute_child\n",
      "\u001b[33m(raylet)\u001b[0m     hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "\u001b[33m(raylet)\u001b[0m PermissionError: [WinError 5] �s���Q�ڡC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-09-23 21:43:00 (running for 00:00:05.34)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 640.000: None | Iter 320.000: None | Iter 160.000: None | Iter 80.000: None | Iter 40.000: None | Iter 20.000: None | Iter 10.000: None | Iter 5.000: None\n",
      "Logical resource usage: 1.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/ian11/AppData/Local/Temp/ray/session_2024-09-23_21-42-49_373180_5416/artifacts/2024-09-23_21-42-54/tune_darts/driver_artifacts\n",
      "Number of trials: 1/30 (1 PENDING)\n",
      "+----------------------+----------+-------+---------------+-----------------------+\n",
      "| Trial name           | status   | loc   |   kernel_size |   lr_scheduler_kwargs |\n",
      "|----------------------+----------+-------+---------------+-----------------------|\n",
      "| train_model_a8f6ff1a | PENDING  |       |            33 |            0.00126183 |\n",
      "+----------------------+----------+-------+---------------+-----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-09-23 21:43:05 (running for 00:00:10.40)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 640.000: None | Iter 320.000: None | Iter 160.000: None | Iter 80.000: None | Iter 40.000: None | Iter 20.000: None | Iter 10.000: None | Iter 5.000: None\n",
      "Logical resource usage: 1.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/ian11/AppData/Local/Temp/ray/session_2024-09-23_21-42-49_373180_5416/artifacts/2024-09-23_21-42-54/tune_darts/driver_artifacts\n",
      "Number of trials: 1/30 (1 PENDING)\n",
      "+----------------------+----------+-------+---------------+-----------------------+\n",
      "| Trial name           | status   | loc   |   kernel_size |   lr_scheduler_kwargs |\n",
      "|----------------------+----------+-------+---------------+-----------------------|\n",
      "| train_model_a8f6ff1a | PENDING  |       |            33 |            0.00126183 |\n",
      "+----------------------+----------+-------+---------------+-----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-09-23 21:43:10 (running for 00:00:15.46)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 640.000: None | Iter 320.000: None | Iter 160.000: None | Iter 80.000: None | Iter 40.000: None | Iter 20.000: None | Iter 10.000: None | Iter 5.000: None\n",
      "Logical resource usage: 1.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/ian11/AppData/Local/Temp/ray/session_2024-09-23_21-42-49_373180_5416/artifacts/2024-09-23_21-42-54/tune_darts/driver_artifacts\n",
      "Number of trials: 1/30 (1 PENDING)\n",
      "+----------------------+----------+-------+---------------+-----------------------+\n",
      "| Trial name           | status   | loc   |   kernel_size |   lr_scheduler_kwargs |\n",
      "|----------------------+----------+-------+---------------+-----------------------|\n",
      "| train_model_a8f6ff1a | PENDING  |       |            33 |            0.00126183 |\n",
      "+----------------------+----------+-------+---------------+-----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-09-23 21:43:15 (running for 00:00:20.54)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 640.000: None | Iter 320.000: None | Iter 160.000: None | Iter 80.000: None | Iter 40.000: None | Iter 20.000: None | Iter 10.000: None | Iter 5.000: None\n",
      "Logical resource usage: 1.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/ian11/AppData/Local/Temp/ray/session_2024-09-23_21-42-49_373180_5416/artifacts/2024-09-23_21-42-54/tune_darts/driver_artifacts\n",
      "Number of trials: 1/30 (1 PENDING)\n",
      "+----------------------+----------+-------+---------------+-----------------------+\n",
      "| Trial name           | status   | loc   |   kernel_size |   lr_scheduler_kwargs |\n",
      "|----------------------+----------+-------+---------------+-----------------------|\n",
      "| train_model_a8f6ff1a | PENDING  |       |            33 |            0.00126183 |\n",
      "+----------------------+----------+-------+---------------+-----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 21:43:18,122\tWARNING tune.py:229 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2024-09-23 21:43:18,128\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-09-23 21:43:18,132\tINFO tune.py:1016 -- Wrote the latest version of all result files and experiment state to 'C:/Users/ian11/ray_results/tune_darts' in 0.0080s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-09-23 21:43:18 (running for 00:00:23.45)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 640.000: None | Iter 320.000: None | Iter 160.000: None | Iter 80.000: None | Iter 40.000: None | Iter 20.000: None | Iter 10.000: None | Iter 5.000: None\n",
      "Logical resource usage: 1.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/ian11/AppData/Local/Temp/ray/session_2024-09-23_21-42-49_373180_5416/artifacts/2024-09-23_21-42-54/tune_darts/driver_artifacts\n",
      "Number of trials: 1/30 (1 PENDING)\n",
      "+----------------------+----------+-------+---------------+-----------------------+\n",
      "| Trial name           | status   | loc   |   kernel_size |   lr_scheduler_kwargs |\n",
      "|----------------------+----------+-------+---------------+-----------------------|\n",
      "| train_model_a8f6ff1a | PENDING  |       |            33 |            0.00126183 |\n",
      "+----------------------+----------+-------+---------------+-----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 21:43:28,209\tINFO tune.py:1048 -- Total run time: 33.65 seconds (23.44 seconds for the tuning loop).\n",
      "2024-09-23 21:43:28,211\tWARNING tune.py:1063 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: tune.run(..., resume=True)\n",
      "2024-09-23 21:43:28,218\tWARNING experiment_analysis.py:190 -- Failed to fetch metrics for 1 trial(s):\n",
      "- train_model_a8f6ff1a: FileNotFoundError('Could not fetch metrics for train_model_a8f6ff1a: both result.json and progress.csv were not found at C:/Users/ian11/ray_results/tune_darts/train_model_a8f6ff1a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  {'kernel_size': 33, 'lr_scheduler_kwargs': 0.0012618337846478911}\n"
     ]
    }
   ],
   "source": [
    "# Run Ray Tune, optimize hyperparameters by minimizing the MAPE on the validation set\n",
    "num_samples = 30\n",
    "\n",
    "scheduler = ASHAScheduler(max_t=1000, grace_period=5, reduction_factor=2)\n",
    "\n",
    "train_fn_with_parameters = tune.with_parameters(\n",
    "    train_model, callbacks=[my_stopper, tune_callback], train=train_series, val=val_series,\n",
    ")\n",
    "\n",
    "analysis = tune.run(\n",
    "    train_fn_with_parameters,\n",
    "    #resources_per_trial=resources_per_trial,\n",
    "    metric=\"loss\",  # any value in TuneReportCallback.\n",
    "    mode=\"min\",\n",
    "    config=config,\n",
    "    num_samples=num_samples,\n",
    "    search_alg=optuna_search,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter,\n",
    "    trial_dirname_creator=lambda trial: str(trial),\n",
    "    name=\"tune_darts\",\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters found were: \", analysis.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[0;32m      2\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39miteritems \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mitems\n\u001b[0;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m analysis\u001b[38;5;241m.\u001b[39mresults_df\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-09-23 21:43:54,925 E 29176 25916] (raylet.exe) worker_pool.cc:550: Some workers of the worker process(25964) have not registered within the timeout. The process is dead, probably it crashed during start.\n"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "pd.DataFrame.iteritems = pd.DataFrame.items\n",
    "\n",
    "df = analysis.results_df\n",
    "\n",
    "# 假設 df 是你的 DataFrame\n",
    "fig = px.parallel_coordinates(df, \n",
    "                              dimensions=['config/kernel_size', 'config/lr_scheduler_kwargs', 'loss'],\n",
    "                              color='loss',\n",
    "                              labels={\"config/kernel_size\": \"Kernel Size\",\n",
    "                                      \"config/lr_scheduler_kwargs\": \"Learning Rate\",\n",
    "                                      \"loss\": \"Loss\"},\n",
    "                              color_continuous_scale=px.colors.diverging.Tealrose,  # 色彩範圍\n",
    "                              #color_continuous_midpoint=0.004\n",
    "                             )  # 中間點，根據數據適當調整\n",
    "\n",
    "# 設定每個維度的範圍\n",
    "fig.update_traces(dimensions=[\n",
    "    dict(range=[5, 100], label='Kernel Size', values=df['config/kernel_size']),\n",
    "    dict(range=[0, 0.01], label='Learning Rate', values=df['config/lr_scheduler_kwargs']),\n",
    "    dict(range=[min(df['loss']), max(df['loss'])], label='Loss', values=df['loss'])\n",
    "])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('C:\\\\Users\\\\ian11\\\\EDtimeseriesForecast\\\\EDtimeseriesForecast\\\\Result\\\\DLinear\\\\Chiayi\\\\Hypertuning.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "EDforecasts",
   "language": "python",
   "name": "edforecasts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
